import{Q as r,a as n}from"./index.esm.7420b775.js";import{Q as l}from"./QPage.d83ce4b9.js";import{_ as h,m as c,q as d,s as m,t as u,u as g,z as e,X as s,v as o,y as a}from"./index.47c864f7.js";import"./QBtn.ace210e5.js";import"./dom.cf56c019.js";import"./position-engine.25adee15.js";import"./selection.3861cc93.js";import"./scroll.95183e25.js";const p=c({name:"PhysarumPage",components:{QMarkdown:r},data(){return{split:60,markdown:`\`\`\`
float sense(float x, float y, float r, float offset)
{
    float sensorAngle = r + offset;
    vec2 sensorDirection = vec2(cos(sensorAngle), sin(sensorAngle));
    vec2 sensorCentre = vec2(x, y) + sensorDirection * sensorOffsetDistance;

    float sum = 0.0;
    for (float i = -sensorWidth; i <= sensorWidth; i += 1.0/width)
    {
        for (float j = -sensorHeight; j <= sensorHeight; j += 1.0/height)
        {
            vec4 reading = texture(renderTex, sensorCentre + vec2(i, j));
            sum += reading.r + reading.g + reading.b;
        }
    }
    return sum / maxPossibleReading;
}
\`\`\``}}}),f={class:"q-mt-none q-mb-lg text-primary"},_=e("div",{class:"text-h4"},"Physarum Polycephalum Simulation",-1),v=e("br",null,null,-1),w=e("p",null,[a(" I learned a lot of the conceptual details for this project from "),e("a",{href:"http://cargocollective.com/sagejenson/physarum",class:"text-accent"},"Sage Jenson's write-up"),a(" on the topic as well as "),e("a",{href:"https://www.youtube.com/watch?v=X-iSQQgOd1A",class:"text-accent"}," Sebastian Lague's video"),a(". ")],-1),y=[w],x=e("p",null," Physarum Polycephalum is a single-cell organism which, within large networks, can together exhibit many complex and intelligent locomotive behaviours. The project involved creating webGL shaders to simulate 100,000s of 'agents', each with a set of simple navigational rules, on the GPU to create complex collective behaviour. ",-1),b=e("p",null," Each agent (or particle) moves based on 3 sensor readings around it (front left, front & front right). The sensors each read the brightness of the pixel they are currently on, and the agent will then move in the direction of the brightest reading (or randomly if two are equal or all read dark). ",-1),P=e("p",null,` The agent then deposits some material onto the pixel it is currently on, which increases that pixel's brightness. The entire trail map then "decays" (is darkened slightly) and dissipated (blurred) using a simple 3x3 blur algorithm. `,-1),k=e("p",null," Each sensor reads pixel values in a small box surrounding it, and that normalised value is compared between the 3 sensors. ",-1),q=e("div",{class:"text-caption"}," Excerpt from the agent movement shader: Given the agent position and sensor angle and offset, returns a normalised reading of the pixel values in the area around a sensor. ",-1),j=e("div",{class:"text-h6"},"Dynamic Agent Speed",-1),Q=e("p",null,` A feature which I wanted to focus on, which I had not seen much of elsewhere, was giving the agents dynamic speeds based on their surroundings. Here, an agent's acceleration is proportional to the brightness of the readings around it. This causes them to form "highways" of high-speed lanes with many agents moving through, and regions agents slowing moving into empty territory. Here, the agent's acceleration also affects their colour. `,-1),T=e("p",null," I doubt that this feature is scientifically accurate to the real-life molds though, which seem to move pretty slowly constantly. ",-1);function $(t,A,C,I,S,D){const i=g("q-markdown");return d(),m(l,null,{default:u(()=>[e("div",f,[_,v,e("div",{class:s(["focused",t.$q.platform.is.mobile?"":"aside"])},y,2),x,o(n,{src:"portfolio/slime_gif_0.gif",class:s(t.$q.platform.is.mobile?"":"aside")},null,8,["class"]),b,P,k,o(i,{style:{"line-height":"1.5",overflow:"auto","overflow-x":"auto","min-width":"0px","box-sizing":"border-box"},src:t.markdown,class:"focused"},null,8,["src"]),q,j,o(n,{src:"portfolio/slime_gif_1.gif",class:s(t.$q.platform.is.mobile?"":"aside")},null,8,["class"]),Q,T])]),_:1})}var O=h(p,[["render",$]]);export{O as default};
